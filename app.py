import streamlit as st
import tempfile
import os
from outils_khawla import extract_text
from qdrant_client import QdrantClient
from langchain_groq import ChatGroq
from qdrant_client.models import VectorParams, Distance
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain.chains import RetrievalQA
from constants import *
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
from prompts import *

# üéØ Initialisation Qdrant 
client = QdrantClient("https://08d055a0-1eb2-45c5-9e9a-64e741bba5ec.europe-west3-0.gcp.cloud.qdrant.io"
, api_key="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.Fau1FEDosaMc3jteRJi2TtaGgq33VxjbFcY6_p3p8w0")

# Charger le mod√®le d'embeddings
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

vector_size = embedding_model.client.get_sentence_embedding_dimension()

if not client.collection_exists(QDRANT_COLLECTION):
    client.create_collection(
        collection_name=QDRANT_COLLECTION,
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),
    )

vectorstore = Qdrant(client=client, collection_name=QDRANT_COLLECTION, embeddings=embedding_model)

# üî• Chargement du mod√®le Groq
# Mod√®les Groq
llm = ChatGroq(groq_api_key="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", model_name="llama3-8b-8192")

# üìå Interface Streamlit
st.set_page_config(page_title="üß† AI Assistant", layout="wide")

st.sidebar.header("üìÇ Chargez vos fichiers : ")
uploaded_file = st.sidebar.file_uploader("Choisir un fichier", type=["pdf", "mp3", "mp4"])

st.sidebar.header("üåç Selectionnez la langue de reponse :")
lang = st.sidebar.selectbox("Langue", ["", "Fran√ßais", "Anglais", "Arabe"])  # Option vide par d√©faut

# üåü Boutons pour action
col1, col2 = st.columns(2)
with col1:
    summarize_btn = st.button("üìù G√©n√©rer un R√©sum√©")
with col2:
    ask_btn = st.button("üí¨ Poser une Question")



def process_and_store_file(file):
    """Extrait et stocke le texte du fichier dans Qdrant."""
    
    # V√©rifie si le fichier est un objet BytesIO (cas de Streamlit)
    if hasattr(file, "read"):
        suffix = os.path.splitext(file.name)[1]  # R√©cup√®re l'extension du fichier
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as temp_file:
            temp_file.write(file.read())  # Sauvegarde temporairement le fichier
            temp_file_path = temp_file.name  # R√©cup√®re le chemin

    else:
        temp_file_path = file  # Si c'est d√©j√† un chemin, on le garde tel quel
    try:
        text = extract_text(temp_file_path)
        if text:
            metadata = {"file_name": file.name, "file_type": file.type}
            vectorstore.add_texts([text], metadatas=[metadata])
            st.success(f"‚úÖ Texte extrait et stock√© depuis {file.name}")
    except ValueError as e:
        st.error(f"‚ö†Ô∏è Erreur lors de l'extraction : {e}")
    finally:
        os.remove(temp_file_path)  # Supprime le fichier temporaire apr√®s traitement

def retrieve_context_with_metadata(query):
    """R√©cup√®re les documents pertinents"""
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    retrieved_docs = retriever.invoke(query)
    return "\n\n".join([f"üìÇ {doc.metadata.get('file_name', 'Inconnu')}\n{doc.page_content}" for doc in retrieved_docs])

chain_resumer = ({"context": itemgetter("context"), "language": itemgetter("language")} | prompt_resumer | llm | StrOutputParser())
chain_chat = ({"context": itemgetter("context"), "question": itemgetter("question"), "language": itemgetter("language")} | prompt_chat | llm | StrOutputParser())

# ‚ö†Ô∏è Condition pour ex√©cuter les actions seulement si la langue est choisie et un bouton est cliqu√©
if lang and uploaded_file:
    if summarize_btn or ask_btn:
        process_and_store_file(uploaded_file)
        
        if summarize_btn:
            query = "Fais un r√©sum√© structur√© des informations disponibles."
            context = retrieve_context_with_metadata(query)
            result = chain_resumer.invoke({"context": context, "language": lang})
            st.subheader("üìù R√©sum√© g√©n√©r√©")
            st.write(result)
        
        if ask_btn:
            query = st.text_input("‚ùì Pose ta question ici")
            if query:
                st.write("üîç Question pos√©e :", query)  # Debug

                context = retrieve_context_with_metadata(query)
                st.write("üìÇ Contexte r√©cup√©r√© :", context)  # Debug

                result = chain_chat.invoke({"context": context, "question": query, "language": lang})
                st.write("ü§ñ R√©ponse g√©n√©r√©e :", result)  # Debug

                st.subheader("ü§ñ R√©ponse du LLM")
                st.write(result)

else:
    st.warning("‚ö†Ô∏è Veuillez t√©l√©verser un fichier et choisir une langue avant de poursuivre.")

ChatGroq(groq_api_key="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", model_name="llama3-8b-8192")
